{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jqaUio_oaxcn"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt \n",
        "import seaborn as sns\n",
        "from sklearn import datasets\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Linear Regression Model"
      ],
      "metadata": {
        "id": "E8HYiWqPd_De"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear_Regression(nn.Module):\n",
        "  def __init__(self, n_inputs, n_outputs):\n",
        "    super(Linear_Regression, self).__init__()\n",
        "    self.regression_model = nn.Linear(n_features, n_outputs)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    y_pred = self.regression_model(x)\n",
        "    return y_pred\n",
        "\n",
        "model = Linear_Regression(n_features, 1)\n",
        "learning_rate = 0.01\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "epochs=100\n",
        "\n",
        "for epoch in range(epochs):\n",
        " \n",
        "    y_pred = model(X_train)\n",
        "  \n",
        "      \n",
        "    loss = criterion(y_pred, y_train)\n",
        "  \n",
        "      \n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    print('epoch:', {epoch+1}, 'loss:', loss.item())\n",
        "\n",
        " \n",
        "  \n",
        "  \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WXwrBC5ga3cZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Accuracy"
      ],
      "metadata": {
        "id": "U4MZU-NSJSOc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_predicted = model(X_train)\n",
        "y_predicted = y_predicted.round()\n",
        "\n",
        "correct = 0\n",
        "len_y = y_train.size(0)\n",
        "for i in range(len_y):\n",
        "  if y_train[i] == y_predicted[i]:\n",
        "    correct+=1\n",
        "print((correct/len_y) * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2yCDYMjJCdl",
        "outputId": "c077cbe3-529c-4b2b-d095-b9ab6164831d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "93.62637362637362\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing Accuracy"
      ],
      "metadata": {
        "id": "lJVvOsJOJU54"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_predicted = model(X_test)\n",
        "y_predicted = y_predicted.round()\n",
        "\n",
        "correct = 0\n",
        "len_y = y_test.size(0)\n",
        "for i in range(len_y):\n",
        "  if y_test[i] == y_predicted[i]:\n",
        "    correct+=1\n",
        "print((correct/len_y) * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KkK2iNWI79O",
        "outputId": "13254a97-3702-4923-94ca-5f1953be5ff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "90.35087719298247\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Logistics Regression"
      ],
      "metadata": {
        "id": "CbyxpU4n50VJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Logistics_Regression(nn.Module):\n",
        "    def __init__(self, n_input_features):\n",
        "        super(Logistics_Regression, self).__init__()\n",
        "        self.linear = nn.Linear(n_input_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y_pred = torch.sigmoid(self.linear(x))\n",
        "        return y_pred\n",
        "\n",
        "model = Logistics_Regression(n_features)\n",
        "\n",
        "\n",
        "num_epochs = 100\n",
        "learning_rate = 0.01\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    y_pred = model(X_train)\n",
        "    loss = criterion(y_pred, y_train)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "     # calculate the derivates of weights and biases\n",
        "     # dw = (1/n_samples)*(np.dot(X.T, (pred - y)))\n",
        "     # db = (1/n_samples)*(np.sum(pred-y))\n",
        "    loss.backward()\n",
        "    \n",
        "    # self.weights = self.weights - self.lr*dw\n",
        "    # self.bias = self.bias - self.lr*db\n",
        "    optimizer.step()\n",
        "\n",
        "\n",
        "    \n",
        "    print('epoch:', {epoch+1}, 'loss:', loss.item())\n",
        "\n",
        "y_predicted = model(X_test)\n",
        "y_predicted = y_predicted.round()\n"
      ],
      "metadata": {
        "id": "FE0eTnD451lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training accuracy"
      ],
      "metadata": {
        "id": "pSWLrTskJYNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_predicted = model(X_train)\n",
        "y_predicted = y_predicted.round()\n",
        "\n",
        "correct = 0\n",
        "len_y = y_train.size(0)\n",
        "for i in range(len_y):\n",
        "  if y_train[i] == y_predicted[i]:\n",
        "    correct+=1\n",
        "print((correct/len_y) * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrSrvviwJX9v",
        "outputId": "b688a13a-b0b1-4a81-c646-0b04419a5ccc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "95.16483516483515\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test Accuracy"
      ],
      "metadata": {
        "id": "srEwz9h-JYk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_predicted = model(X_test)\n",
        "y_predicted = y_predicted.round()\n",
        "\n",
        "correct = 0\n",
        "len_y = y_test.size(0)\n",
        "for i in range(len_y):\n",
        "  if y_test[i] == y_predicted[i]:\n",
        "    correct+=1\n",
        "print((correct/len_y) * 100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3Dxa-gaHFub",
        "outputId": "cbe5fcaa-abc4-424a-bc57-22df1d140e25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "92.98245614035088\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Decision Tree"
      ],
      "metadata": {
        "id": "tF6EEycFKk4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Decision_Tree(nn.Module):\n",
        "    def __init__(self, n_input_features):\n",
        "        super(Decision_Tree, self).__init__()\n",
        "        self.decision_tree = nn.\n",
        "\n",
        "    def forward(self, x):\n",
        "        y_pred = torch.sigmoid(self.linear(x))\n",
        "        return y_pred\n",
        "\n"
      ],
      "metadata": {
        "id": "872KcAfAMkfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class SDT(nn.Module):\n",
        "    \"\"\"Fast implementation of soft decision tree in PyTorch.\n",
        "    Parameters\n",
        "    ----------\n",
        "    input_dim : int\n",
        "      The number of input dimensions.\n",
        "    output_dim : int\n",
        "      The number of output dimensions. For example, for a multi-class\n",
        "      classification problem with `K` classes, it is set to `K`.\n",
        "    depth : int, default=5\n",
        "      The depth of the soft decision tree. Since the soft decision tree is\n",
        "      a full binary tree, setting `depth` to a large value will drastically\n",
        "      increases the training and evaluating cost.\n",
        "    lamda : float, default=1e-3\n",
        "      The coefficient of the regularization term in the training loss. Please\n",
        "      refer to the paper on the formulation of the regularization term.\n",
        "    use_cuda : bool, default=False\n",
        "      When set to `True`, use GPU to fit the model. Training a soft decision\n",
        "      tree using CPU could be faster considering the inherent data forwarding\n",
        "      process.\n",
        "    Attributes\n",
        "    ----------\n",
        "    internal_node_num_ : int\n",
        "      The number of internal nodes in the tree. Given the tree depth `d`, it\n",
        "      equals to :math:`2^d - 1`.\n",
        "    leaf_node_num_ : int\n",
        "      The number of leaf nodes in the tree. Given the tree depth `d`, it equals\n",
        "      to :math:`2^d`.\n",
        "    penalty_list : list\n",
        "      A list storing the layer-wise coefficients of the regularization term.\n",
        "    inner_nodes : torch.nn.Sequential\n",
        "      A container that simulates all internal nodes in the soft decision tree.\n",
        "      The sigmoid activation function is concatenated to simulate the\n",
        "      probabilistic routing mechanism.\n",
        "    leaf_nodes : torch.nn.Linear\n",
        "      A `nn.Linear` module that simulates all leaf nodes in the tree.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_dim,\n",
        "            output_dim,\n",
        "            depth=5,\n",
        "            lamda=1e-3,\n",
        "            use_cuda=False):\n",
        "        super(SDT, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "        self.depth = depth\n",
        "        self.lamda = lamda\n",
        "        self.device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "        self._validate_parameters()\n",
        "\n",
        "        self.internal_node_num_ = 2 ** self.depth - 1\n",
        "        self.leaf_node_num_ = 2 ** self.depth\n",
        "\n",
        "        # Different penalty coefficients for nodes in different layers\n",
        "        self.penalty_list = [\n",
        "            self.lamda * (2 ** (-depth)) for depth in range(0, self.depth)\n",
        "        ]\n",
        "\n",
        "        # Initialize internal nodes and leaf nodes, the input dimension on\n",
        "        # internal nodes is added by 1, serving as the bias.\n",
        "        self.inner_nodes = nn.Sequential(\n",
        "            nn.Linear(self.input_dim + 1, self.internal_node_num_, bias=False),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "\n",
        "        self.leaf_nodes = nn.Linear(self.leaf_node_num_,\n",
        "                                    self.output_dim,\n",
        "                                    bias=False)\n",
        "\n",
        "    def forward(self, X, is_training_data=False):\n",
        "        _mu, _penalty = self._forward(X)\n",
        "        y_pred = self.leaf_nodes(_mu)\n",
        "\n",
        "        # When `X` is the training data, the model also returns the penalty\n",
        "        # to compute the training loss.\n",
        "        if is_training_data:\n",
        "            return y_pred, _penalty\n",
        "        else:\n",
        "            return y_pred\n",
        "\n",
        "    def _forward(self, X):\n",
        "        \"\"\"Implementation on the data forwarding process.\"\"\"\n",
        "\n",
        "        batch_size = X.size()[0]\n",
        "        X = self._data_augment(X)\n",
        "\n",
        "        path_prob = self.inner_nodes(X)\n",
        "        path_prob = torch.unsqueeze(path_prob, dim=2)\n",
        "        path_prob = torch.cat((path_prob, 1 - path_prob), dim=2)\n",
        "\n",
        "        _mu = X.data.new(batch_size, 1, 1).fill_(1.0)\n",
        "        _penalty = torch.tensor(0.0).to(self.device)\n",
        "\n",
        "        # Iterate through internal odes in each layer to compute the final path\n",
        "        # probabilities and the regularization term.\n",
        "        begin_idx = 0\n",
        "        end_idx = 1\n",
        "\n",
        "        for layer_idx in range(0, self.depth):\n",
        "            _path_prob = path_prob[:, begin_idx:end_idx, :]\n",
        "\n",
        "            # Extract internal nodes in the current layer to compute the\n",
        "            # regularization term\n",
        "            _penalty = _penalty + self._cal_penalty(layer_idx, _mu, _path_prob)\n",
        "            _mu = _mu.view(batch_size, -1, 1).repeat(1, 1, 2)\n",
        "\n",
        "            _mu = _mu * _path_prob  # update path probabilities\n",
        "\n",
        "            begin_idx = end_idx\n",
        "            end_idx = begin_idx + 2 ** (layer_idx + 1)\n",
        "\n",
        "        mu = _mu.view(batch_size, self.leaf_node_num_)\n",
        "\n",
        "        return mu, _penalty\n",
        "\n",
        "    def _cal_penalty(self, layer_idx, _mu, _path_prob):\n",
        "        \"\"\"\n",
        "        Compute the regularization term for internal nodes in different layers.\n",
        "        \"\"\"\n",
        "\n",
        "        penalty = torch.tensor(0.0).to(self.device)\n",
        "\n",
        "        batch_size = _mu.size()[0]\n",
        "        _mu = _mu.view(batch_size, 2 ** layer_idx)\n",
        "        _path_prob = _path_prob.view(batch_size, 2 ** (layer_idx + 1))\n",
        "\n",
        "        for node in range(0, 2 ** (layer_idx + 1)):\n",
        "            alpha = torch.sum(\n",
        "                _path_prob[:, node] * _mu[:, node // 2], dim=0\n",
        "            ) / torch.sum(_mu[:, node // 2], dim=0)\n",
        "\n",
        "            coeff = self.penalty_list[layer_idx]\n",
        "\n",
        "            penalty -= 0.5 * coeff * (torch.log(alpha) + torch.log(1 - alpha))\n",
        "\n",
        "        return penalty\n",
        "\n",
        "    def _data_augment(self, X):\n",
        "        \"\"\"Add a constant input `1` onto the front of each sample.\"\"\"\n",
        "        batch_size = X.size()[0]\n",
        "        X = X.view(batch_size, -1)\n",
        "        bias = torch.ones(batch_size, 1).to(self.device)\n",
        "        X = torch.cat((bias, X), 1)\n",
        "\n",
        "        return X\n",
        "\n",
        "    def _validate_parameters(self):\n",
        "\n",
        "        if not self.depth > 0:\n",
        "            msg = (\"The tree depth should be strictly positive, but got {}\"\n",
        "                   \"instead.\")\n",
        "            raise ValueError(msg.format(self.depth))\n",
        "\n",
        "        if not self.lamda >= 0:\n",
        "            msg = (\n",
        "                \"The coefficient of the regularization term should not be\"\n",
        "                \" negative, but got {} instead.\"\n",
        "            )\n",
        "            raise ValueError(msg.format(self.lamda))"
      ],
      "metadata": {
        "id": "aH5p-HAcN4oG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Breast Cancer Data"
      ],
      "metadata": {
        "id": "I3QJUcut6seY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "breast_cancer_data = datasets.load_breast_cancer()\n",
        "X, y = breast_cancer_data.data, breast_cancer_data.target\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# scale\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "y_train = y_train.view(y_train.shape[0], 1)\n",
        "y_test = y_test.view(y_test.shape[0], 1)"
      ],
      "metadata": {
        "id": "R-HVFtQn6r4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diabetes = datasets.load_breast_cancer()\n",
        "X, y = diabetes.data, diabetes.target\n",
        "\n",
        "df = pd.DataFrame(diabetes,columns=diabetes.feature_names)\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "xoeXCwghD71A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.DataFrame(data_diabetes.data,columns=data_diabetes.feature_names)\n"
      ],
      "metadata": {
        "id": "dmu8qM1CEGgl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape   #569 rows, 30 columns"
      ],
      "metadata": {
        "id": "70p0kuyyEcvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Target']=data_diabetes.target"
      ],
      "metadata": {
        "id": "JNRLr3PsFWbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(30)"
      ],
      "metadata": {
        "id": "zCUgpc4gFY-1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}